{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a74c2e",
   "metadata": {},
   "source": [
    "## Creating a AWS Session using boto3 with an IAM user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fd352ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d62b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = boto3.session.Session(profile_name='iam_user_for_projects')\n",
    "\n",
    "# I configured the iam user through aws cli in local machine. so I need not to give credentials here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca812f16",
   "metadata": {},
   "source": [
    "## Creating a s3 buckets to store the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec02740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the s3 client\n",
    "\n",
    "s3 = console.client(service_name='s3',region_name='ap-south-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "452cbc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket has been created\n"
     ]
    }
   ],
   "source": [
    "# creating the s3 bucket for the raw csv files\n",
    "\n",
    "location_name = \"ap-south-1\"\n",
    "bucket_name = \"raw-csv-from-jupyter\"\n",
    "location = {'LocationConstraint': location_name}\n",
    "\n",
    "try:\n",
    "    bucket = s3.create_bucket(Bucket=bucket_name,CreateBucketConfiguration=location)\n",
    "    print(\"Bucket has been created\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d9b9d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket has been created\n"
     ]
    }
   ],
   "source": [
    "# creating the s3 bucket for the raw json files\n",
    "\n",
    "location_name = \"ap-south-1\"\n",
    "bucket_name = \"raw-json-from-jupyter\"\n",
    "location = {'LocationConstraint': location_name}\n",
    "\n",
    "try:\n",
    "    bucket = s3.create_bucket(Bucket=bucket_name,CreateBucketConfiguration=location)\n",
    "    print(\"Bucket has been created\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a37c8fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-csv-from-jupyter\n",
      "raw-json-from-jupyter\n"
     ]
    }
   ],
   "source": [
    "for i in s3.list_buckets().get('Buckets'):\n",
    "    print(i['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "476f9491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Uploaded\n"
     ]
    }
   ],
   "source": [
    "# uploading the raw csv files to the s3 bucket\n",
    "\n",
    "s3.upload_file(Filename=r'(C:\\Users\\BALA\\Youtube ETL Project\\csv_files\\CAvideos.csv)', Bucket='raw-csv-from-jupyter', Key='CAvideos.csv')\n",
    "s3.upload_file(Filename=r'(C:\\Users\\BALA\\Youtube ETL Project\\csv_files\\DEvideos.csv)', Bucket='raw-csv-from-jupyter', Key='DEvideos.csv')\n",
    "s3.upload_file(Filename=r'(C:\\Users\\BALA\\Youtube ETL Project\\csv_files\\FRvideos.csv)', Bucket='raw-csv-from-jupyter', Key='FRvideos.csv')\n",
    "s3.upload_file(Filename=r'(C:\\Users\\BALA\\Youtube ETL Project\\csv_files\\GBvideos.csv)', Bucket='raw-csv-from-jupyter', Key='GBvideos.csv')\n",
    "s3.upload_file(Filename=r'(C:\\Users\\BALA\\Youtube ETL Project\\csv_files\\INvideos.csv)', Bucket='raw-csv-from-jupyter', Key='INvideos.csv')\n",
    "s3.upload_file(Filename=r'(C:\\Users\\BALA\\Youtube ETL Project\\csv_files\\JPvideos.csv)', Bucket='raw-csv-from-jupyter', Key='JPvideos.csv')\n",
    "s3.upload_file(Filename=r'(C:\\Users\\BALA\\Youtube ETL Project\\csv_files\\KRvideos.csv)', Bucket='raw-csv-from-jupyter', Key='KRvideos.csv')\n",
    "s3.upload_file(Filename=r'(C:\\Users\\BALA\\Youtube ETL Project\\csv_files\\MXvideos.csv)', Bucket='raw-csv-from-jupyter', Key='MXvideos.csv')\n",
    "s3.upload_file(Filename=r'(C:\\Users\\BALA\\Youtube ETL Project\\csv_files\\RUvideos.csv)', Bucket='raw-csv-from-jupyter', Key='RUvideos.csv')\n",
    "s3.upload_file(Filename=r'(C:\\Users\\BALA\\Youtube ETL Project\\csv_files\\USvideos.csv)', Bucket='raw-csv-from-jupyter', Key='USvideos.csv')\n",
    "print('Successfully Uploaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c50cb7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Uploaded\n"
     ]
    }
   ],
   "source": [
    "# uploading the raw json files to the s3 bucket\n",
    "\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\CA_category_id.json'), Bucket='raw-json-from-jupyter', Key='CA_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\DE_category_id.json'), Bucket='raw-json-from-jupyter', Key='DE_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\FR_category_id.json'), Bucket='raw-json-from-jupyter', Key='FR_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\GB_category_id.json'), Bucket='raw-json-from-jupyter', Key='GB_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\IN_category_id.json'), Bucket='raw-json-from-jupyter', Key='IN_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\JP_category_id.json'), Bucket='raw-json-from-jupyter', Key='JP_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\KR_category_id.json'), Bucket='raw-json-from-jupyter', Key='KR_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\MX_category_id.json'), Bucket='raw-json-from-jupyter', Key='MX_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\RU_category_id.json'), Bucket='raw-json-from-jupyter', Key='RU_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\US_category_id.json'), Bucket='raw-json-from-jupyter', Key='US_category_id.json')\n",
    "print('Successfully uploaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "85bf00bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA_category_id.json\n",
      "DE_category_id.json\n",
      "FR_category_id.json\n",
      "GB_category_id.json\n",
      "IN_category_id.json\n",
      "JP_category_id.json\n",
      "KR_category_id.json\n",
      "MX_category_id.json\n",
      "RU_category_id.json\n",
      "US_category_id.json\n"
     ]
    }
   ],
   "source": [
    "for i in s3.list_objects(Bucket='raw-json-from-jupyter')['Contents']:\n",
    "    print(i['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8b5aea67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAvideos.csv\n",
      "DEvideos.csv\n",
      "FRvideos.csv\n",
      "GBvideos.csv\n",
      "INvideos.csv\n",
      "JPvideos.csv\n",
      "KRvideos.csv\n",
      "MXvideos.csv\n",
      "RUvideos.csv\n",
      "USvideos.csv\n"
     ]
    }
   ],
   "source": [
    "for i in s3.list_objects(Bucket='raw-csv-from-jupyter')['Contents']:\n",
    "    print(i['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2957ee",
   "metadata": {},
   "source": [
    "# Creating glue crawler to investigate the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57332fa8",
   "metadata": {},
   "source": [
    "## creating a role for glue and attaching required permission policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49bac947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the IAM client\n",
    "\n",
    "iam = console.client(service_name='iam',region_name='ap-south-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48f4ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON file for the IAM role for glue\n",
    "\n",
    "import json\n",
    "document=json.dumps({\n",
    " \"Version\": \"2012-10-17\",\n",
    " \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\":{'Service':'glue.amazonaws.com'},\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8fe0a8ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Role': {'Path': '/',\n",
       "  'RoleName': 'role-for-glue',\n",
       "  'RoleId': 'AROA5AKEZQQNRMA7ZBCQL',\n",
       "  'Arn': 'arn:aws:iam::894034347035:role/role-for-glue',\n",
       "  'CreateDate': datetime.datetime(2022, 12, 20, 8, 51, 14, tzinfo=tzutc()),\n",
       "  'AssumeRolePolicyDocument': {'Version': '2012-10-17',\n",
       "   'Statement': [{'Effect': 'Allow',\n",
       "     'Principal': {'Service': 'glue.amazonaws.com'},\n",
       "     'Action': 'sts:AssumeRole'}]}},\n",
       " 'ResponseMetadata': {'RequestId': 'cd16675f-5ea0-4686-b635-4515e236c732',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'cd16675f-5ea0-4686-b635-4515e236c732',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '778',\n",
       "   'date': 'Tue, 20 Dec 2022 08:51:14 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the IAM role with the above json file\n",
    "\n",
    "iam.create_role(RoleName='role-for-glue',\n",
    "               AssumeRolePolicyDocument=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "55a6812f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '29805cd8-82dc-4c17-8b45-fef668a17f98',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '29805cd8-82dc-4c17-8b45-fef668a17f98',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '212',\n",
       "   'date': 'Tue, 20 Dec 2022 09:07:05 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attaching the required permissio policies with the role. AmazonS3FullAccess and AWSGlueServiceRole\n",
    "\n",
    "iam.attach_role_policy(RoleName='role-for-glue',PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess')\n",
    "iam.attach_role_policy(RoleName='role-for-glue',PolicyArn='arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afdd7a8",
   "metadata": {},
   "source": [
    "## Creating Glue crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9579c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Glue client\n",
    "\n",
    "glue = console.client(service_name='glue',region_name='ap-south-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "73785f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully Created\n"
     ]
    }
   ],
   "source": [
    "# Creating a glue crawler to populate a table on the json files.\n",
    "\n",
    "try:  \n",
    "    glue.create_crawler(Name='crawler1',\n",
    "                        Role='role-for-glue',\n",
    "                        DatabaseName='crawler_db',\n",
    "                        Targets={'S3Targets':[{'Path':'s3://raw-json-from-jupyter/json_files/'}]})\n",
    "    print('Succesfully Created')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91819f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the crawler\n",
    "\n",
    "try:\n",
    "    glue.start_crawler(Name='crawler1')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f3b31f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'kind', 'Type': 'string'},\n",
       " {'Name': 'etag', 'Type': 'string'},\n",
       " {'Name': 'items',\n",
       "  'Type': 'array<struct<kind:string,etag:string,id:string,snippet:struct<channelId:string,title:string,assignable:boolean>>>'}]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the Schema of the table populated on the raw json files.\n",
    "\n",
    "glue_tables['TableList'][0]['StorageDescriptor']['Columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0e55e094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind  string\n",
      "etag  string\n",
      "items  array<struct<kind:string,etag:string,id:string,snippet:struct<channelId:string,title:string,assignable:boolean>>>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "glue_tables = glue.get_tables(DatabaseName='crawler_db')\n",
    "\n",
    "for table in glue_tables['TableList']:\n",
    "    for column in table['StorageDescriptor']['Columns']:\n",
    "        column_name = column['Name']\n",
    "        comment = column.get('Comment', '')\n",
    "        column_type = column['Type']\n",
    "        print(column_name,comment,column_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a829a769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully Created\n"
     ]
    }
   ],
   "source": [
    "# Creating the crawler for to populate a table on raw csv files.\n",
    "\n",
    "try:  \n",
    "    glue.create_crawler(Name='crawler2',\n",
    "                        Role='role-for-glue',\n",
    "                        DatabaseName='crawler_db_for_csv',\n",
    "                        Targets={'S3Targets':[{'Path':'s3://raw-csv-from-jupyter'}]})\n",
    "    print('Succesfully Created')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c3b14f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the crawler\n",
    "\n",
    "try:\n",
    "    glue.start_crawler(Name='crawler2')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15d3854a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'video_id', 'Type': 'string'},\n",
       " {'Name': 'trending_date', 'Type': 'string'},\n",
       " {'Name': 'title', 'Type': 'string'},\n",
       " {'Name': 'channel_title', 'Type': 'string'},\n",
       " {'Name': 'category_id', 'Type': 'bigint'},\n",
       " {'Name': 'publish_time', 'Type': 'string'},\n",
       " {'Name': 'tags', 'Type': 'string'},\n",
       " {'Name': 'views', 'Type': 'bigint'},\n",
       " {'Name': 'likes', 'Type': 'bigint'},\n",
       " {'Name': 'dislikes', 'Type': 'bigint'},\n",
       " {'Name': 'comment_count', 'Type': 'bigint'},\n",
       " {'Name': 'thumbnail_link', 'Type': 'string'},\n",
       " {'Name': 'comments_disabled', 'Type': 'boolean'},\n",
       " {'Name': 'ratings_disabled', 'Type': 'boolean'},\n",
       " {'Name': 'video_error_or_removed', 'Type': 'boolean'},\n",
       " {'Name': 'description', 'Type': 'string'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the Schema of the table on the raw csv file.\n",
    "\n",
    "glue.get_tables(DatabaseName='crawler_db_for_csv')['TableList'][0]['StorageDescriptor']['Columns']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28589ce5",
   "metadata": {},
   "source": [
    "# What's about the raw data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66eee7a",
   "metadata": {},
   "source": [
    "## What I came to know about the raw json and csv files is,The JSON files are nested ones. We can't read a nested json in columnar format. So, we need to flatten the nested json.And there is nothing much to worry about the csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b691fd4",
   "metadata": {},
   "source": [
    "# What need to be done with the raw data to get the final analytical data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a73add",
   "metadata": {},
   "source": [
    "## So, the plan is to merge the csv and json files based on the id columns of both the files. We have issue with the json file. So, we need to flatten the json and change the data type of the id column on json from string to integer inorder to merge with the category_id column of csv file which is in integer datatype. I'm going to use Lambda function to procees the json files and Glue to merge both the files to get the final data for analytics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e79a86",
   "metadata": {},
   "source": [
    "# Creating Lambda function for processing JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e17006b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Lambda client\n",
    "\n",
    "lamda = console.client(service_name='lambda',region_name='ap-south-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d870096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the json file for creating the IAM role for Lambda\n",
    "\n",
    "document=json.dumps({\n",
    " \"Version\": \"2012-10-17\",\n",
    " \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\":{'Service':'lambda.amazonaws.com'},\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6e257081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Role': {'Path': '/',\n",
       "  'RoleName': 'role-for-lambda',\n",
       "  'RoleId': 'AROA5AKEZQQN356KPWKNF',\n",
       "  'Arn': 'arn:aws:iam::894034347035:role/role-for-lambda',\n",
       "  'CreateDate': datetime.datetime(2022, 12, 21, 6, 59, 25, tzinfo=tzutc()),\n",
       "  'AssumeRolePolicyDocument': {'Version': '2012-10-17',\n",
       "   'Statement': [{'Effect': 'Allow',\n",
       "     'Principal': {'Service': 'lambda.amazonaws.com'},\n",
       "     'Action': 'sts:AssumeRole'}]}},\n",
       " 'ResponseMetadata': {'RequestId': '211f8c81-8df2-412c-be1c-0def8a3bbfb5',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '211f8c81-8df2-412c-be1c-0def8a3bbfb5',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '784',\n",
       "   'date': 'Wed, 21 Dec 2022 06:59:25 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating role for lambda with the above file.\n",
    "\n",
    "iam.create_role(RoleName='role-for-lambda',\n",
    "               AssumeRolePolicyDocument=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "63db3896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'a7f68a2c-b61e-442e-8e27-5f59dbd1038d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'a7f68a2c-b61e-442e-8e27-5f59dbd1038d',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '212',\n",
       "   'date': 'Wed, 21 Dec 2022 07:10:45 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attaching the required policies to the role. LambdaBasicExecution policy and s3FullAccess policy\n",
    "\n",
    "iam.attach_role_policy(RoleName='role-for-lambda',PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess')\n",
    "iam.attach_role_policy(RoleName='role-for-lambda',PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "098620a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket has been created\n"
     ]
    }
   ],
   "source": [
    "# Creating bucket for the lambda function script.\n",
    "\n",
    "location_name = \"ap-south-1\"\n",
    "bucket_name = \"bucket-for-lambda-function\"\n",
    "location = {'LocationConstraint': location_name}\n",
    "\n",
    "try:\n",
    "    bucket = s3.create_bucket(Bucket=bucket_name,CreateBucketConfiguration=location)\n",
    "    print(\"Bucket has been created\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d6b54e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket has been created\n"
     ]
    }
   ],
   "source": [
    "# Creating s3 bucket for the cleaned json files which are processed by Lambda\n",
    "\n",
    "location_name = \"ap-south-1\"\n",
    "bucket_name = \"cleaned-json-as-csv\"\n",
    "location = {'LocationConstraint': location_name}\n",
    "\n",
    "try:\n",
    "    bucket = s3.create_bucket(Bucket=bucket_name,CreateBucketConfiguration=location)\n",
    "    print(\"Bucket has been created\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe9f27f",
   "metadata": {},
   "source": [
    "## Lambda Function Script \n",
    " What i am doing here in the lambda script is, first getting the name and key of the files from the event.\n",
    " \n",
    " Using the name and key getting the object from the bucket.\n",
    " \n",
    " Through pandas read_json method I rad the json file in the object.\n",
    " \n",
    " Then I normalised the nested json and changed the data type of id column in order to merge with the csv file.bcz in csv  file category id is in integer data type.\n",
    " \n",
    " Then I written the dataframe as csv file in the cleaned json bucket using the buffer and put object method.\n",
    " \n",
    " And I put the csv file inside a folder inorder to create partitioned table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2db85",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "s3 = boto3.resource(service_name='s3',region_name='ap-south-1')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    bucketname = event['Records'][0]['s3']['bucket']['name']\n",
    "    filename = event['Records'][0]['s3']['object']['key']   \n",
    "\n",
    "    obj = s3.Object(bucket_name=bucketname,key=filename).get()\n",
    "\n",
    "    df = pd.read_json(obj['Body'])             # reading the json file from the object\n",
    "\n",
    "    df = pd.json_normalize(df['items'])        # Flattening the nested json\n",
    "\n",
    "    df['id']= df['id'].astype('int64')         # Changing the datatype of the id column\n",
    "\n",
    "    csv_buffer = StringIO()\n",
    "\n",
    "    df.to_csv(csv_buffer,index=0)              # Converting as csv ang putting in the buffer\n",
    "    \n",
    "    f_name = filename[:-17]+'/'+filename[:-5]  # Its to put the file under a folder. Its to create partitioned table.\n",
    "\n",
    "    s3.Object(bucket_name='cleaned-json-as-csv',key=f_name).put(Body=csv_buffer.getvalue()) # putting in s3 bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e485f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lambda script file need to be uploaded to the bucket in the bytes format.So converting the script in bytes format.\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "with open((r'C:\\Users\\BALA\\aws_deployment_package\\lambda_function.zip'), \"rb\") as f:\n",
    "    buf = BytesIO(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bc92aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'KFNCE0FHBG50NDD2',\n",
       "  'HostId': 'NvRFxZNstsZ6Y5drbO546cHbQrvzKYCFCYs8MGHkqxGn1W4Fh0UQvv135iP24arAD7+Cv1lhkj0=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'NvRFxZNstsZ6Y5drbO546cHbQrvzKYCFCYs8MGHkqxGn1W4Fh0UQvv135iP24arAD7+Cv1lhkj0=',\n",
       "   'x-amz-request-id': 'KFNCE0FHBG50NDD2',\n",
       "   'date': 'Sun, 25 Dec 2022 12:51:25 GMT',\n",
       "   'etag': '\"656228c8c123ed9565b8ab1e539b8631\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"656228c8c123ed9565b8ab1e539b8631\"'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Putting the script in the s3 bucket.\n",
    "\n",
    "s3_resource = boto3.resource(service_name='s3',region_name='ap-south-1')\n",
    "s3_resource.Object(bucket_name='bucket-for-lambda-function',key='lambda_zipfile').put(Body=buf.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a2c960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'e9168ae3-7d19-4d03-b962-f883439558aa',\n",
       "  'HTTPStatusCode': 201,\n",
       "  'HTTPHeaders': {'date': 'Sun, 25 Dec 2022 13:01:58 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '1027',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'e9168ae3-7d19-4d03-b962-f883439558aa'},\n",
       "  'RetryAttempts': 0},\n",
       " 'FunctionName': 'lambda-function-for-json-processing',\n",
       " 'FunctionArn': 'arn:aws:lambda:ap-south-1:894034347035:function:lambda-function-for-json-processing',\n",
       " 'Runtime': 'python3.9',\n",
       " 'Role': 'arn:aws:iam::894034347035:role/role-for-lambda',\n",
       " 'Handler': 'lambda_function.lambda_handler',\n",
       " 'CodeSize': 519,\n",
       " 'Description': '',\n",
       " 'Timeout': 3,\n",
       " 'MemorySize': 128,\n",
       " 'LastModified': '2022-12-25T13:01:58.861+0000',\n",
       " 'CodeSha256': 'LYIH/kIs2MW67kREYPvrSIyaewBRzC2XwzASc1XVE3M=',\n",
       " 'Version': '$LATEST',\n",
       " 'TracingConfig': {'Mode': 'PassThrough'},\n",
       " 'RevisionId': '748632df-7117-4424-9ab5-4ecc9921b089',\n",
       " 'State': 'Pending',\n",
       " 'StateReason': 'The function is being created.',\n",
       " 'StateReasonCode': 'Creating',\n",
       " 'PackageType': 'Zip',\n",
       " 'Architectures': ['x86_64'],\n",
       " 'EphemeralStorage': {'Size': 512}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the Lambda function.\n",
    "\n",
    "lamda.create_function(\n",
    "        FunctionName='lambda-function-for-json-processing',\n",
    "        Code={'S3Bucket': 'bucket-for-lambda-function',\n",
    "              'S3Key': 'lambda_zipfile'},\n",
    "        Role='arn:aws:iam::894034347035:role/role-for-lambda',\n",
    "        Handler='lambda_function.lambda_handler',\n",
    "        Runtime='python3.9'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5ec8c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda Function Successfully Invoked\n"
     ]
    }
   ],
   "source": [
    "# Invoking the function.\n",
    "\n",
    "try:\n",
    "    lamda.invoke(FunctionName='lambda-function-for-json-processing')\n",
    "    print('Lambda Function Successfully Invoked')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "221fd379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded\n"
     ]
    }
   ],
   "source": [
    "# Uploading the json files again in the raw s3 bucket. Uploading the file will trigger the Lambda function.\n",
    "\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\CA_category_id.json'), Bucket='raw-json-from-jupyter', Key='CA_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\DE_category_id.json'), Bucket='raw-json-from-jupyter', Key='DE_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\FR_category_id.json'), Bucket='raw-json-from-jupyter', Key='FR_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\GB_category_id.json'), Bucket='raw-json-from-jupyter', Key='GB_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\IN_category_id.json'), Bucket='raw-json-from-jupyter', Key='IN_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\JP_category_id.json'), Bucket='raw-json-from-jupyter', Key='JP_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\KR_category_id.json'), Bucket='raw-json-from-jupyter', Key='KR_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\MX_category_id.json'), Bucket='raw-json-from-jupyter', Key='MX_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\RU_category_id.json'), Bucket='raw-json-from-jupyter', Key='RU_category_id.json')\n",
    "s3.upload_file(Filename=(r'C:\\Users\\BALA\\Youtube ETL Project\\json_files\\US_category_id.json'), Bucket='raw-json-from-jupyter', Key='US_category_id.json')\n",
    "print('Successfully uploaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4e1b7c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA_category_id.json\n",
      "DE_category_id.json\n",
      "FR_category_id.json\n",
      "GB_category_id.json\n",
      "IN_category_id.json\n",
      "JP_category_id.json\n",
      "KR_category_id.json\n",
      "MX_category_id.json\n",
      "RU_category_id.json\n",
      "US_category_id.json\n"
     ]
    }
   ],
   "source": [
    "# Those json files are uploaded into the raw bucket.\n",
    "\n",
    "for i in s3.list_objects(Bucket='raw-json-from-jupyter')['Contents']:\n",
    "    print(i['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da94e84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA/CA_category_id\n",
      "DE/DE_category_id\n",
      "FR/FR_category_id\n",
      "GB/GB_category_id\n",
      "IN/IN_category_id\n",
      "JP/JP_category_id\n",
      "KR/KR_category_id\n",
      "MX/MX_category_id\n",
      "RU/RU_category_id\n",
      "US/US_category_id\n"
     ]
    }
   ],
   "source": [
    "# See, the cleaned json files in csv format uploaded in the mentioned path in the cleaned json s3 bucket. \n",
    "\n",
    "for i in s3.list_objects(Bucket='cleaned-json-as-csv')['Contents']:\n",
    "    print(i['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc99c7",
   "metadata": {},
   "source": [
    "## Investigating the cleaned json files through the Glue crawler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "46d40ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully Created\n"
     ]
    }
   ],
   "source": [
    "try:  \n",
    "    glue.create_crawler(Name='crawler3',\n",
    "                        Role='role-for-glue',\n",
    "                        DatabaseName='crawler_db_for_cleanedjson',\n",
    "                        Targets={'S3Targets':[{'Path':'s3://cleaned-json-as-csv'}]})\n",
    "    print('Succesfully Created')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "480229d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    glue.start_crawler(Name='crawler3')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90cd73",
   "metadata": {},
   "source": [
    "## You can see the flattened json in the csv format in the below cell. Now we can access all the columns. And also you can see the data type of id column is bigint and also category id column of csv file in bigint datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8eb80142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'kind', 'Type': 'string'},\n",
       " {'Name': 'etag', 'Type': 'string'},\n",
       " {'Name': 'id', 'Type': 'bigint'},\n",
       " {'Name': 'snippet.channelid', 'Type': 'string'},\n",
       " {'Name': 'snippet.title', 'Type': 'string'},\n",
       " {'Name': 'snippet.assignable', 'Type': 'boolean'}]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue.get_tables(DatabaseName='crawler_db_for_cleanedjson')['TableList'][0]['StorageDescriptor']['Columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bc61a5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'video_id', 'Type': 'string'},\n",
       " {'Name': 'trending_date', 'Type': 'string'},\n",
       " {'Name': 'title', 'Type': 'string'},\n",
       " {'Name': 'channel_title', 'Type': 'string'},\n",
       " {'Name': 'category_id', 'Type': 'bigint'},\n",
       " {'Name': 'publish_time', 'Type': 'string'},\n",
       " {'Name': 'tags', 'Type': 'string'},\n",
       " {'Name': 'views', 'Type': 'bigint'},\n",
       " {'Name': 'likes', 'Type': 'bigint'},\n",
       " {'Name': 'dislikes', 'Type': 'bigint'},\n",
       " {'Name': 'comment_count', 'Type': 'bigint'},\n",
       " {'Name': 'thumbnail_link', 'Type': 'string'},\n",
       " {'Name': 'comments_disabled', 'Type': 'boolean'},\n",
       " {'Name': 'ratings_disabled', 'Type': 'boolean'},\n",
       " {'Name': 'video_error_or_removed', 'Type': 'boolean'},\n",
       " {'Name': 'description', 'Type': 'string'}]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue.get_tables(DatabaseName='crawler_db_for_csv')['TableList'][0]['StorageDescriptor']['Columns']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a2c57",
   "metadata": {},
   "source": [
    "# Glue job for creating the final data for analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a57ac60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket has been created\n"
     ]
    }
   ],
   "source": [
    "# Creating the bucket of the glue script.\n",
    "\n",
    "location_name = \"ap-south-1\"\n",
    "bucket_name = \"bucket-for-glue-script\"\n",
    "location = {'LocationConstraint': location_name}\n",
    "\n",
    "try:\n",
    "    bucket = s3.create_bucket(Bucket=bucket_name,CreateBucketConfiguration=location)\n",
    "    print(\"Bucket has been created\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a9091fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket has been created\n"
     ]
    }
   ],
   "source": [
    "# Creating the s3 bucket for the final processed data by glue\n",
    "\n",
    "location_name = \"ap-south-1\"\n",
    "bucket_name = \"analytical-data-bucket\"\n",
    "location = {'LocationConstraint': location_name}\n",
    "\n",
    "try:\n",
    "    bucket = s3.create_bucket(Bucket=bucket_name,CreateBucketConfiguration=location)\n",
    "    print(\"Bucket has been created\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ab121",
   "metadata": {},
   "source": [
    "## Glue script\n",
    "## The idea is, we have the glue catelogs for both csv and cleaned json files. Here i'm going to create dynamic frames from the catelogs. And merging them with the Join method based on the id and category_id columns of both the frames.Then I write the joined frame in csv format in the final s3 bucket with the partition of regions. so that we can do analysis easily within the region partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e3a674",
   "metadata": {},
   "source": [
    " import sys\n",
    "\n",
    " from awsglue.transforms import *\n",
    "\n",
    " from awsglue.utils import getResolvedOptions\n",
    "\n",
    " from pyspark.context import SparkContext\n",
    "\n",
    " from awsglue.context import GlueContext\n",
    "\n",
    " from awsglue.job import Job\n",
    " \n",
    "\n",
    " args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    " sc = SparkContext()\n",
    "\n",
    " glueContext = GlueContext(sc)\n",
    "\n",
    " spark = glueContext.spark_session\n",
    "\n",
    " job = Job(glueContext)\n",
    "\n",
    " job.init(args['JOB_NAME'], args)\n",
    " \n",
    "#creating dynamic frame from the json file catelog. \n",
    "\n",
    " cleaned_json_frame = glueContext.create_dynamic_frame.from_catalog ( database = \"crawler_db_for_cleanedjson\",\n",
    "\n",
    "                                                                table_name=\"cleaned_json_as_csv\",\n",
    "                                                                \n",
    "                                                                transformation_ctx=\"cleaned_json_frame\")\n",
    "\n",
    "#creating dynamic frame from the csv file catelog. \n",
    "\n",
    " raw_csv_frame = glueContext.create_dynamic_frame.from_catalog ( database = \"crawler_db_for_csv\",\n",
    "\n",
    "                                                                table_name=\"raw_csv_from_jupyter\",\n",
    "                                                                \n",
    "                                                                transformation_ctx=\"raw_csv_frame\")\n",
    "#Joining both the dynamic frames based on the id columns.\n",
    "\n",
    " joined_frame = join.apply(frame1 = cleaned_json_frame,\n",
    "\n",
    "                        frame2 = raw_csv_frame,\n",
    "                        \n",
    "                        keys1 = ['category_id'],\n",
    "                        \n",
    "                        keys2 = ['id']\n",
    "                        \n",
    "                        transformation_ctx=\"joined_frame\")\n",
    "                        \n",
    "#writing the joined frame into the s3 bucket with the partitions.\n",
    "\n",
    " S3_target = glueContext.getSink(path=\"s3://analytical-data-bucket\",\n",
    "\n",
    "                                connection_type=\"s3\",\n",
    "                                \n",
    "                                updateBehavior=\"LOG\",\n",
    "                                \n",
    "                                partitionKeys=[\"partition_0\"],\n",
    "                                \n",
    "                                enableUpdateCatalog=True,\n",
    "                                \n",
    "                                transformation_ctx=\"S3_target\")\n",
    "\n",
    " S3_target.setFormat(\"csv\")\n",
    "\n",
    " S3_target.writeFrame(joined_frame)\n",
    "\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "492cfa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded Successfully\n"
     ]
    }
   ],
   "source": [
    "# uploading the glue script to the s3 bucket\n",
    "\n",
    "try:\n",
    "    s3.upload_file(Filename=(r'C:\\Users\\BALA\\aws_deployment_package\\spark_script_for_glue.py'),\n",
    "                   Bucket='bucket-for-glue-script', Key='Scripts/spark_script_for_glue.py')\n",
    "    print('Uploaded Successfully')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "28d3ac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scripts/spark_script_for_glue.py\n"
     ]
    }
   ],
   "source": [
    "for i in s3.list_objects(Bucket='bucket-for-glue-script')['Contents']:\n",
    "    print(i['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4d82eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Job created\n"
     ]
    }
   ],
   "source": [
    "# Creating the glue job\n",
    "\n",
    "try:\n",
    "    glue.create_job(Name='glue-job-for-join',\n",
    "                    Role='arn:aws:iam::894034347035:role/role-for-glue',\n",
    "                    Command={'Name':'glueetl',\n",
    "                        'ScriptLocation':'s3://bucket-for-glue-script/Scripts/spark_script_for_glue'\n",
    "                            'PythonVersion': '3'},\n",
    "                    MaxRetries=0,\n",
    "                    Timeout=2880,\n",
    "                    GlueVersion='3.0',\n",
    "                    NumberOfWorkers=2,\n",
    "                    WorkerType='G.1X')\n",
    "\n",
    "    print('Successfully Job created')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e956ed14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JobRunId': 'jr_45f79044d9317a51efb44f5c7371c3d38d64d4bfcf95200e1f99e73b37b62288',\n",
       " 'ResponseMetadata': {'RequestId': '758d2ba7-413e-43a4-9657-1c4cfe036e18',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Mon, 02 Jan 2023 06:54:29 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '82',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '758d2ba7-413e-43a4-9657-1c4cfe036e18'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting the glue job.\n",
    "\n",
    "glue.start_job_run(JobName='glue-job-for-join')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3aed1ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition_0=CA/run-AmazonS3_node1672643785456-1-part-r-00000\n",
      "partition_0=CA/run-AmazonS3_node1672643785456-1-part-r-00001\n",
      "partition_0=CA/run-AmazonS3_node1672643785456-1-part-r-00002\n",
      "partition_0=CA/run-AmazonS3_node1672643785456-1-part-r-00003\n",
      "partition_0=DE/run-AmazonS3_node1672643785456-1-part-r-00000\n",
      "partition_0=DE/run-AmazonS3_node1672643785456-1-part-r-00001\n",
      "partition_0=DE/run-AmazonS3_node1672643785456-1-part-r-00002\n",
      "partition_0=DE/run-AmazonS3_node1672643785456-1-part-r-00003\n",
      "partition_0=FR/run-AmazonS3_node1672643785456-1-part-r-00000\n",
      "partition_0=FR/run-AmazonS3_node1672643785456-1-part-r-00001\n",
      "partition_0=FR/run-AmazonS3_node1672643785456-1-part-r-00002\n",
      "partition_0=FR/run-AmazonS3_node1672643785456-1-part-r-00003\n",
      "partition_0=GB/run-AmazonS3_node1672643785456-1-part-r-00000\n",
      "partition_0=GB/run-AmazonS3_node1672643785456-1-part-r-00001\n",
      "partition_0=GB/run-AmazonS3_node1672643785456-1-part-r-00002\n",
      "partition_0=GB/run-AmazonS3_node1672643785456-1-part-r-00003\n",
      "partition_0=IN/run-AmazonS3_node1672643785456-1-part-r-00000\n",
      "partition_0=IN/run-AmazonS3_node1672643785456-1-part-r-00001\n",
      "partition_0=IN/run-AmazonS3_node1672643785456-1-part-r-00002\n",
      "partition_0=IN/run-AmazonS3_node1672643785456-1-part-r-00003\n",
      "partition_0=JP/run-AmazonS3_node1672643785456-1-part-r-00000\n",
      "partition_0=JP/run-AmazonS3_node1672643785456-1-part-r-00001\n",
      "partition_0=JP/run-AmazonS3_node1672643785456-1-part-r-00002\n",
      "partition_0=JP/run-AmazonS3_node1672643785456-1-part-r-00003\n",
      "partition_0=KR/run-AmazonS3_node1672643785456-1-part-r-00000\n",
      "partition_0=KR/run-AmazonS3_node1672643785456-1-part-r-00001\n",
      "partition_0=KR/run-AmazonS3_node1672643785456-1-part-r-00002\n",
      "partition_0=KR/run-AmazonS3_node1672643785456-1-part-r-00003\n",
      "partition_0=MX/run-AmazonS3_node1672643785456-1-part-r-00000\n",
      "partition_0=MX/run-AmazonS3_node1672643785456-1-part-r-00001\n",
      "partition_0=MX/run-AmazonS3_node1672643785456-1-part-r-00002\n",
      "partition_0=MX/run-AmazonS3_node1672643785456-1-part-r-00003\n",
      "partition_0=RU/run-AmazonS3_node1672643785456-1-part-r-00000\n",
      "partition_0=RU/run-AmazonS3_node1672643785456-1-part-r-00001\n",
      "partition_0=RU/run-AmazonS3_node1672643785456-1-part-r-00002\n",
      "partition_0=RU/run-AmazonS3_node1672643785456-1-part-r-00003\n",
      "partition_0=US/run-AmazonS3_node1672643785456-1-part-r-00000\n",
      "partition_0=US/run-AmazonS3_node1672643785456-1-part-r-00001\n",
      "partition_0=US/run-AmazonS3_node1672643785456-1-part-r-00002\n",
      "partition_0=US/run-AmazonS3_node1672643785456-1-part-r-00003\n"
     ]
    }
   ],
   "source": [
    "# you can see the final processed data in the form of blocks in the analytical data bucket\n",
    "\n",
    "for i in s3.list_objects(Bucket='analytical-data-bucket')['Contents']:\n",
    "    print(i['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbea2f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully Created\n"
     ]
    }
   ],
   "source": [
    "# Creating a crawler for the analytical data bucket\n",
    "\n",
    "try:  \n",
    "    glue.create_crawler(Name='crawler_db_for_analytical_data',\n",
    "                        Role='role-for-glue',\n",
    "                        DatabaseName='analytical_db',\n",
    "                        Targets={'S3Targets':[{'Path':'s3://analytical-data-bucket'}]})\n",
    "    print('Succesfully Created')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2d7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the crawler\n",
    "\n",
    "try:\n",
    "    glue.start_crawler(Name='crawler_db_for_analytical_data')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554ba1ca",
   "metadata": {},
   "source": [
    "## See the final data with all the required columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70b71b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'ratings_disabled', 'Type': 'boolean'},\n",
       " {'Name': 'snippet.title', 'Type': 'string'},\n",
       " {'Name': 'comments_disabled', 'Type': 'boolean'},\n",
       " {'Name': 'trending_date', 'Type': 'string'},\n",
       " {'Name': 'etag', 'Type': 'string'},\n",
       " {'Name': 'video_id', 'Type': 'string'},\n",
       " {'Name': 'category_id', 'Type': 'bigint'},\n",
       " {'Name': 'thumbnail_link', 'Type': 'string'},\n",
       " {'Name': 'kind', 'Type': 'string'},\n",
       " {'Name': 'likes', 'Type': 'bigint'},\n",
       " {'Name': 'comment_count', 'Type': 'bigint'},\n",
       " {'Name': 'snippet.channelid', 'Type': 'string'},\n",
       " {'Name': 'description', 'Type': 'string'},\n",
       " {'Name': 'views', 'Type': 'bigint'},\n",
       " {'Name': 'dislikes', 'Type': 'bigint'},\n",
       " {'Name': 'snippet.assignable', 'Type': 'boolean'},\n",
       " {'Name': 'channel_title', 'Type': 'string'},\n",
       " {'Name': 'title', 'Type': 'string'},\n",
       " {'Name': '.partition_0', 'Type': 'string'},\n",
       " {'Name': 'publish_time', 'Type': 'string'},\n",
       " {'Name': 'tags', 'Type': 'string'},\n",
       " {'Name': 'id', 'Type': 'bigint'},\n",
       " {'Name': 'video_error_or_removed', 'Type': 'boolean'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue.get_tables(DatabaseName='analytical_db')['TableList'][0]['StorageDescriptor']['Columns']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa3692",
   "metadata": {},
   "source": [
    "# Athena for querying the analytical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8af938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client successfully created\n"
     ]
    }
   ],
   "source": [
    "# Creating athena client\n",
    "\n",
    "try:\n",
    "    athena = console.client(service_name='athena',region_name='ap-south-1')\n",
    "    print('Client successfully created')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42404bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON file for the IAM role for Arhena\n",
    "\n",
    "import json\n",
    "document1=json.dumps({\n",
    " \"Version\": \"2012-10-17\",\n",
    " \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\":{'Service':'athena.amazonaws.com'},\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "535dd7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Role': {'Path': '/',\n",
       "  'RoleName': 'role-for-athena',\n",
       "  'RoleId': 'AROA5AKEZQQNWZGG3ORIB',\n",
       "  'Arn': 'arn:aws:iam::894034347035:role/role-for-athena',\n",
       "  'CreateDate': datetime.datetime(2023, 1, 3, 6, 44, 50, tzinfo=tzutc()),\n",
       "  'AssumeRolePolicyDocument': {'Version': '2012-10-17',\n",
       "   'Statement': [{'Effect': 'Allow',\n",
       "     'Principal': {'Service': 'athena.amazonaws.com'},\n",
       "     'Action': 'sts:AssumeRole'}]}},\n",
       " 'ResponseMetadata': {'RequestId': '9138d11c-ce14-424d-8620-04c4843a3841',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '9138d11c-ce14-424d-8620-04c4843a3841',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '784',\n",
       "   'date': 'Tue, 03 Jan 2023 06:44:50 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the role\n",
    "\n",
    "iam.create_role(RoleName='role-for-athena',\n",
    "               AssumeRolePolicyDocument=document1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "446ea0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'bc7f08b0-e7d5-47e0-9f60-95df1fb4193b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'bc7f08b0-e7d5-47e0-9f60-95df1fb4193b',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '212',\n",
       "   'date': 'Tue, 03 Jan 2023 06:45:48 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attaching the required permission policy for Athena. AmazonS3FullAccess policy.\n",
    "\n",
    "iam.attach_role_policy(RoleName='role-for-athena',PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a056d6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket has been created\n"
     ]
    }
   ],
   "source": [
    "# Creating a s3 bucket for the Athena query result.\n",
    "\n",
    "location_name = \"ap-south-1\"\n",
    "bucket_name = \"bucket-athena-result\"\n",
    "location = {'LocationConstraint': location_name}\n",
    "\n",
    "try:\n",
    "    bucket = s3.create_bucket(Bucket=bucket_name,CreateBucketConfiguration=location)\n",
    "    print(\"Bucket has been created\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460eeb4a",
   "metadata": {},
   "source": [
    "# Querying the analytical db data\n",
    "\n",
    "## Here I'm creating a query for getting the video details of top 10 most liked videos in a the region.\n",
    "\n",
    "## The Query:\n",
    "\n",
    "## SELECT \"title\", \"snippet.title\",\"likes\",\"partition_0\" FROM \"analytical_db\".\"analytical_data_bucket\" WHERE \"partition_0\"='CA' ORDER BY \"likes\" DESC limit 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ae5f2d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QueryExecutionId': '4bcf5341-9f53-4367-940b-1dfa3620a0d7',\n",
       " 'ResponseMetadata': {'RequestId': 'bafb3bb3-57cc-4882-a5fe-548b56d0a4ae',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 04 Jan 2023 03:54:42 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '59',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'bafb3bb3-57cc-4882-a5fe-548b56d0a4ae'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# querying the table in Athena\n",
    "\n",
    "athena.start_query_execution(QueryString='''SELECT \"title\", \"snippet.title\",\"likes\",\"partition_0\" FROM \"analytical_db\".\"analytical_data_bucket\" WHERE \"partition_0\"='CA' ORDER BY \"likes\" DESC limit 10;''',\n",
    "                            QueryExecutionContext={'Database':'analytical_db'},\n",
    "                             ResultConfiguration={'OutputLocation':'s3://bucket-athena-result'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "817eb940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'VarCharValue': 'title'}, {'VarCharValue': 'snippet.title'}, {'VarCharValue': 'likes'}, {'VarCharValue': 'partition_0'}]\n",
      "[{'VarCharValue': ' KoreaConnect with BTS:http://www.ibighit.com http://twitter.com/BTS_bighit http://twitter.com/BTS_twt https://www.facebook.com/ibighit/http://www.facebook.com/bangtan.officialhttp://instagram.com/BTS.bighitofficialhttp://weibo.com/BTSbighitBU content certified by Big Hit Entertainment\"'}, {'VarCharValue': 'Music'}, {'VarCharValue': '5150839'}, {'VarCharValue': 'CA'}]\n",
      "[{'VarCharValue': ' KoreaConnect with BTS:http://www.ibighit.com http://twitter.com/BTS_bighit http://twitter.com/BTS_twt https://www.facebook.com/ibighit/http://www.facebook.com/bangtan.officialhttp://instagram.com/BTS.bighitofficialhttp://weibo.com/BTSbighitBU content certified by Big Hit Entertainment\"'}, {'VarCharValue': 'Music'}, {'VarCharValue': '5053338'}, {'VarCharValue': 'CA'}]\n",
      "[{'VarCharValue': ' KoreaConnect with BTS:http://www.ibighit.com http://twitter.com/BTS_bighit http://twitter.com/BTS_twt https://www.facebook.com/ibighit/http://www.facebook.com/bangtan.officialhttp://instagram.com/BTS.bighitofficialhttp://weibo.com/BTSbighitBU content certified by Big Hit Entertainment\"'}, {'VarCharValue': 'Music'}, {'VarCharValue': '4924056'}, {'VarCharValue': 'CA'}]\n",
      "[{'VarCharValue': ' KoreaConnect with BTS:http://www.ibighit.com http://twitter.com/BTS_bighit http://twitter.com/BTS_twt https://www.facebook.com/ibighit/http://www.facebook.com/bangtan.officialhttp://instagram.com/BTS.bighitofficialhttp://weibo.com/BTSbighitBU content certified by Big Hit Entertainment\"'}, {'VarCharValue': 'Music'}, {'VarCharValue': '4924056'}, {'VarCharValue': 'CA'}]\n",
      "[{'VarCharValue': ' KoreaConnect with BTS:http://www.ibighit.com http://twitter.com/BTS_bighit http://twitter.com/BTS_twt https://www.facebook.com/ibighit/http://www.facebook.com/bangtan.officialhttp://instagram.com/BTS.bighitofficialhttp://weibo.com/BTSbighitBU content certified by Big Hit Entertainment\"'}, {'VarCharValue': 'Music'}, {'VarCharValue': '4750254'}, {'VarCharValue': 'CA'}]\n",
      "[{'VarCharValue': ' KoreaConnect with BTS:http://www.ibighit.com http://twitter.com/BTS_bighit http://twitter.com/BTS_twt https://www.facebook.com/ibighit/http://www.facebook.com/bangtan.officialhttp://instagram.com/BTS.bighitofficialhttp://weibo.com/BTSbighitBU content certified by Big Hit Entertainment\"'}, {'VarCharValue': 'Music'}, {'VarCharValue': '4750254'}, {'VarCharValue': 'CA'}]\n",
      "[{'VarCharValue': ' KoreaConnect with BTS:http://www.ibighit.com http://twitter.com/BTS_bighit http://twitter.com/BTS_twt https://www.facebook.com/ibighit/http://www.facebook.com/bangtan.officialhttp://instagram.com/BTS.bighitofficialhttp://weibo.com/BTSbighitBU content certified by Big Hit Entertainment\"'}, {'VarCharValue': 'Music'}, {'VarCharValue': '4470923'}, {'VarCharValue': 'CA'}]\n",
      "[{'VarCharValue': ' KoreaConnect with BTS:http://www.ibighit.com http://twitter.com/BTS_bighit http://twitter.com/BTS_twt https://www.facebook.com/ibighit/http://www.facebook.com/bangtan.officialhttp://instagram.com/BTS.bighitofficialhttp://weibo.com/BTSbighitBU content certified by Big Hit Entertainment\"'}, {'VarCharValue': 'Music'}, {'VarCharValue': '4470923'}, {'VarCharValue': 'CA'}]\n",
      "[{'VarCharValue': ' KoreaConnect with BTS:http://www.ibighit.com http://twitter.com/BTS_bighit http://twitter.com/BTS_twt https://www.facebook.com/ibighit/http://www.facebook.com/bangtan.officialhttp://instagram.com/BTS.bighitofficialhttp://weibo.com/BTSbighitBU content certified by Big Hit Entertainment\"'}, {'VarCharValue': 'Music'}, {'VarCharValue': '4470923'}, {'VarCharValue': 'CA'}]\n",
      "[{'VarCharValue': ' KoreaConnect with BTS:http://www.ibighit.com http://twitter.com/BTS_bighit http://twitter.com/BTS_twt https://www.facebook.com/ibighit/http://www.facebook.com/bangtan.officialhttp://instagram.com/BTS.bighitofficialhttp://weibo.com/BTSbighitBU content certified by Big Hit Entertainment\"'}, {'VarCharValue': 'Music'}, {'VarCharValue': '4470923'}, {'VarCharValue': 'CA'}]\n"
     ]
    }
   ],
   "source": [
    "# getting the query result with the QueryExecutionId.\n",
    "\n",
    "a = athena.get_query_results(QueryExecutionId='4bcf5341-9f53-4367-940b-1dfa3620a0d7')['ResultSet']['Rows']\n",
    "for i in a:\n",
    "    print(i['Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4787a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
